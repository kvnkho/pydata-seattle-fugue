{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Distributed Computing (10 mins)\n",
    "\n",
    "Before diving into code, let's first take a look at the the current tooling out there, and the use cases that demand distributed computing.\n",
    "\n",
    "In this section, we explore:\n",
    "\n",
    "* when do I need distributed computing?\n",
    "* is big data still a thing?\n",
    "* what does the big data ecosystem look like?\n",
    "* what are the issues with current frameworks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When Do I Use Distributed Computing?\n",
    "\n",
    "pandas is great for small datasets, but unfortunately does not scale well large datasets. The primary reason is that pandas is single core, and does not take advantage of all available compute resources. A lot of operations also generate [intermediate copies](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#scaling-to-large-datasets) of data, utilizing more memory than necessary. To effectively handle data with pandas, users preferably need to have [5x to 10x times](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) as much RAM as the size of the dataset.\n",
    "\n",
    "Spark and Dask allow us to split compute jobs across multiple machines. They also can handle datasets that donâ€™t fit into memory by [spilling data](http://distributed.dask.org/en/latest/worker.html#spill-data-to-disk) over to disk in some cases.\n",
    "\n",
    "<img src=\"https://ml.dask.org/_images/dimensions_of_scale.svg\" align=\"left\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Big Data dying?\n",
    "\n",
    "## Can't I just use Polars and DuckDB?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computing Architecture\n",
    "\n",
    "There is an image in the Dask repo [issues](https://github.com/dask/dask/issues/4471) that clearly illustrates the distributed computing paradigm. In general, there is a client or master that takes care of the orchestration and final data collection. The client is responsible for scheduling tasks among workers.\n",
    "\n",
    "Both Spark and Dask have local modes also where they use the cores available on the local machine. This means we can still take advantage of the additional processing without having a cluster available.\n",
    "\n",
    "In the diagram below, note how:\n",
    "- package versions and serialization\n",
    "- reading in files can be optimized\n",
    "- data actually lives on a physical machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/11656932/62263986-bbba2f00-b3e3-11e9-9b5c-8446ba4efcf9.png\" align=\"left\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductions to Partitions\n",
    "\n",
    "In order to understand partitions, we can look at this image showing the way Dask scales Pandas. Each partition is a Pandas DataFrame. A Dask DataFrame is the collection of all of the Pandas DataFrames. Operations are done on each partition, and then aggregated back.\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask-dataframe.svg\" align=\"left\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Tools\n",
    "\n",
    "Spark\n",
    "\n",
    "Dask\n",
    "\n",
    "Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with Distributed Computing\n",
    "\n",
    "1. Expertise Required\n",
    "\n",
    "2. Different Syntax\n",
    "\n",
    "3. Hard to Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
