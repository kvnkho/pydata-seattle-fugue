{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Distributed Computing (10 mins)\n",
    "\n",
    "[![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](http://slack.fugue.ai)\n",
    "\n",
    "Before diving into code, let's first take a look at the the current tooling out there, and the use cases that demand distributed computing.\n",
    "\n",
    "In this section, we explore:\n",
    "\n",
    "* when do I need distributed computing?\n",
    "* is big data still a thing?\n",
    "* what does the big data ecosystem look like?\n",
    "* what are the issues with current frameworks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=!mamba install -y openjdk\n",
    "_=!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When We Need Distributed Computing\n",
    "\n",
    "pandas is great for small datasets, but unfortunately does not scale well large datasets. The primary reason is that pandas is single core, and does not take advantage of all available compute resources. A lot of operations also generate [intermediate copies](https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html#scaling-to-large-datasets) of data, utilizing more memory than necessary. To effectively handle data with pandas, users preferably need to have [5x to 10x times](https://wesmckinney.com/blog/apache-arrow-pandas-internals/) as much RAM as the size of the dataset.\n",
    "\n",
    "Spark and Dask allow us to split compute jobs across multiple machines. They also can handle datasets that donâ€™t fit into memory by [spilling data](http://distributed.dask.org/en/latest/worker.html#spill-data-to-disk) over to disk in some cases. This feature should be used sparingly though.\n",
    "\n",
    "The `dask-ml` documentation has a good diagram of the types of problems that call for distributed computing. The general advice is to only introduce distributed computng when you need it because of the additional complexity that comes with building and maintaing such solutions (more on this later).\n",
    "\n",
    "<img src=\"https://ml.dask.org/_images/dimensions_of_scale.svg\" align=\"left\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Big Data Dead?\n",
    "\n",
    "![img](../images/polars_duckdb.png)\n",
    "\n",
    "There was an article by [MotherDuck](https://motherduck.com/blog/big-data-is-dead/) saying big data is dead. There are a lot of reasons, but the one most relevant to us is that tools like [Polars](https://github.com/pola-rs/polars) and [DuckDB](https://github.com/duckdb/duckdb) allow users to process significantly larger amounts of data on a single machine. It is true that tooling on the local ecosystem is better, but as shown in the graph above, speed can also be a concern. We can still use distributed computing for use cases like training multiple machine learning models over a cluster.\n",
    "\n",
    "Second, there are still large data sources as mentioned by the [rebuttal blog by Ponder](https://ponder.io/big-data-is-dead-long-live-big-data/). Transactional and event-based data can generate a lot of data, and are often underutilized because they are left in cold storage. To utilize this data effectively, we need to leverage big data tools. We don't need big data tooling for all projects, but we do need to leverage them in certain use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computing Architecture\n",
    "\n",
    "There is an image in the Dask repo [issues](https://github.com/dask/dask/issues/4471) that clearly illustrates the distributed computing paradigm. In general, there is a client is responsible for interacting with the scheduler that takes care of the orchestration and final data collection. \n",
    "\n",
    "All Spark, Dask, and Ray have local modes also where they use the cores available on the local machine. This means we can still take advantage of the additional processing without having a cluster available. \n",
    "\n",
    "In the diagram below, note how data actually lives on a physical machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/11656932/62263986-bbba2f00-b3e3-11e9-9b5c-8446ba4efcf9.png\" align=\"left\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductions to Partitions\n",
    "\n",
    "In order to understand partitions, we can look at this image showing the way Dask scales Pandas. Each partition is a Pandas DataFrame. A Dask DataFrame is the collection of all of the Pandas DataFrames. Operations are done on each partition, and then aggregated back.\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask-dataframe.svg\" align=\"left\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Computing Ecosystem \n",
    "\n",
    "![img](../images/spark_dask_ray.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pain Points of Distributed Computing\n",
    "\n",
    "* Iteration is harder (and costlier)\n",
    "* Code can be harder to test\n",
    "* Code is locked in to a framework, and can look very different\n",
    "* Requires expertise to maintain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Differing Syntax\n",
    "\n",
    "Let's see an example where we do a `groupby.median()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>col1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group  col1\n",
       "0     A    57\n",
       "1     B    88\n",
       "2     B    90\n",
       "3     B    53\n",
       "4     B    50"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'group': np.random.choice([\"A\",\"B\"], 50),\n",
    "                   'col1': np.random.randint(1, 100, 50)})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>col1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>55.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>47.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group  col1\n",
       "0     A  55.5\n",
       "1     B  47.5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pandas_median(df):\n",
    "    return df.groupby(\"group\").median()\n",
    "\n",
    "pandas_median(df).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PySpark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sdf = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|group|median|\n",
      "+-----+------+\n",
      "|    B|    43|\n",
      "|    A|    45|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "def spark_median(sdf):\n",
    "    med_func = F.expr('percentile_approx(col1, 0.5, 10)').alias('median')\n",
    "    return sdf.groupBy('group').agg(med_func)\n",
    "\n",
    "spark_median(sdf).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, we look at mapping over a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>col1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Banana</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banana</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banana</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Banana</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    group  col1\n",
       "0   Apple    57\n",
       "1  Banana    88\n",
       "2  Banana    90\n",
       "3  Banana    53\n",
       "4  Banana    50"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper = {\"A\": \"Apple\", \"B\": \"Banana\"}\n",
    "def map_letter_to_food(df):\n",
    "    return df.assign(group=df['group'].replace(mapper))\n",
    "\n",
    "map_letter_to_food(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "| group|col1|\n",
      "+------+----+\n",
      "| Apple|  57|\n",
      "|Banana|  88|\n",
      "|Banana|  90|\n",
      "|Banana|  53|\n",
      "|Banana|  50|\n",
      "+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from pyspark.sql.functions import create_map, lit\n",
    "\n",
    "def map_letter_to_food_spark(sdf):\n",
    "    mapping = create_map([lit(x) for x in chain(*mapper.items())])\n",
    "    return sdf.withColumn(\"group\", mapping[sdf['group']])\n",
    "\n",
    "map_letter_to_food_spark(sdf).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is this Problematic\n",
    "\n",
    "1. Our code becomes framework dependent. We can't reuse the same Spark logic for Pandas-sized projects.\n",
    "2. Even if you have the bandwidth to move everything, there can be different behavior (we'll see more later)\n",
    "3. Code becomes a lot harder to maintain (needs expertise). StackOverflow shows 5x for Pandas users than Spark users.\n",
    "\n",
    "So what if we could have a layer that lets us toggle between Pandas and Spark (or Dask, and Ray)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
